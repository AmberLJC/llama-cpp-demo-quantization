#!/usr/bin/env python3
"""
Visualize quantization experiment results.
Creates charts showing the relationship between layer placement and perplexity.
"""

import json
from pathlib import Path

RESULTS_DIR = Path("/Users/amberljc/Desktop/github-project/llama-cpp-demo/results")

def main():
    # Load results
    with open(RESULTS_DIR / "quantization_results.json") as f:
        results = json.load(f)

    # Filter valid results
    valid = [r for r in results if "error" not in r]

    print("="*80)
    print("QUANTIZATION LAYER PLACEMENT EXPERIMENT - DETAILED ANALYSIS")
    print("="*80)
    print()

    # Sort by perplexity
    sorted_results = sorted(valid, key=lambda x: x["perplexity"])

    print("RANKING BY PERPLEXITY (lower is better):")
    print("-"*80)
    print(f"{'Rank':<5} {'Configuration':<30} {'PPL':<10} {'Size(MB)':<10} {'PPL/Size':<10}")
    print("-"*80)

    baseline_ppl = next(r["perplexity"] for r in results if r["name"] == "baseline_fp16")
    baseline_size = next(r["size_mb"] for r in results if r["name"] == "baseline_fp16")

    for i, r in enumerate(sorted_results, 1):
        ratio = r["perplexity"] / r["size_mb"] * 100  # Efficiency metric
        print(f"{i:<5} {r['name']:<30} {r['perplexity']:<10.4f} {r['size_mb']:<10.2f} {ratio:<10.4f}")

    print()
    print("="*80)
    print("SIZE vs PERPLEXITY TRADE-OFF")
    print("="*80)
    print()

    # Calculate compression ratio and perplexity increase
    print(f"{'Configuration':<30} {'Compression':<12} {'PPL Increase':<15} {'Quality/Compression':<20}")
    print("-"*80)

    for r in sorted_results:
        if r["name"] == "baseline_fp16":
            continue
        compression = baseline_size / r["size_mb"]
        ppl_increase = (r["perplexity"] - baseline_ppl) / baseline_ppl * 100
        # Quality metric: how much compression per 1% perplexity increase
        quality = compression / (ppl_increase + 0.01) if ppl_increase > 0 else float('inf')
        print(f"{r['name']:<30} {compression:<12.2f}x {ppl_increase:<15.2f}% {quality:<20.2f}")

    print()
    print("="*80)
    print("ASCII CHART: Perplexity vs Configuration")
    print("="*80)
    print()

    # Simple ASCII bar chart
    min_ppl = min(r["perplexity"] for r in valid)
    max_ppl = max(r["perplexity"] for r in valid)
    chart_width = 50

    for r in sorted_results:
        name = r["name"][:25].ljust(25)
        ppl = r["perplexity"]
        # Normalize to chart width
        bar_len = int((ppl - min_ppl) / (max_ppl - min_ppl + 0.01) * chart_width)
        bar = "â–ˆ" * max(1, bar_len)
        print(f"{name} |{bar} {ppl:.2f}")

    print()
    print("="*80)
    print("KEY FINDINGS")
    print("="*80)
    print("""
    1. EARLY LAYERS MATTER MOST FOR THIS MODEL
       - Keeping the first 8 layers at Q8 gave the best perplexity (13.07)
       - This suggests early layers capture critical features that degrade
         significantly when quantized to 4-bit.

    2. LATE LAYERS ARE MORE ROBUST TO QUANTIZATION
       - Keeping only the last 4-8 layers at Q8 showed less improvement
       - Last 4 layers Q8: PPL = 13.93 vs First 4 layers Q8: PPL = 13.23
       - Difference of 0.70 perplexity points favoring early layers

    3. FFN LAYERS BENEFIT MORE FROM HIGHER PRECISION
       - FFN at Q8: PPL = 13.31 (Size: 485 MB)
       - Attention at Q8: PPL = 13.82 (Size: 357 MB)
       - Counter-intuitively, FFN weights seem more sensitive here

    4. OPTIMAL STRATEGY: PROTECT EARLY LAYERS
       - For same model size (~460-490 MB), first_8_layers_q8 wins
       - This achieves 2.8x compression with only +1.3% perplexity increase

    5. EFFICIENCY METRIC (Compression per PPL degradation):
       - Best: first_8_layers_q8 (highest quality for given compression)
       - Worst: uniform_q4_0 (most aggressive but most quality loss)
    """)

    print()
    print("="*80)
    print("RECOMMENDATIONS")
    print("="*80)
    print("""
    For practical deployment of Qwen2-0.5B:

    1. MEMORY CONSTRAINED (< 400 MB target):
       Use: uniform_q4_0 (336 MB, PPL 14.16)
       Accept the ~10% perplexity increase

    2. BALANCED (400-500 MB target):
       Use: first_8_layers_q8 (492 MB, PPL 13.07)
       Only ~1.3% perplexity increase with 1.9x compression

    3. QUALITY FOCUSED:
       Use: ffn_q8 or first_4_layers_q8 for different size points

    4. GENERAL RULE:
       When constrained on how many layers to keep at high precision,
       prioritize EARLY layers over LATE layers for this model.
    """)


if __name__ == "__main__":
    main()
